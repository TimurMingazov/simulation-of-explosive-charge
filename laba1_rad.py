# app.py
# Streamlit-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π: –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ (–≤ –±—Ä–∞—É–∑–µ—Ä–µ) + LPC –∞–Ω–∞–ª–∏–∑ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è Word-–æ—Ç—á—ë—Ç–∞
# –ó–ê–ú–ï–ù–ê sounddevice -> streamlit-webrtc (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ Streamlit, —Ç.–∫. –º–∏–∫—Ä–æ—Ñ–æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –±—Ä–∞—É–∑–µ—Ä—É)

import streamlit as st
from scipy.io.wavfile import write, read
import scipy.signal as signal
import numpy as np
import matplotlib.pyplot as plt
import os
from dataclasses import dataclass
from typing import Optional
from datetime import datetime
import tempfile
import time
import pandas as pd
from docxtpl import DocxTemplate, InlineImage
from docx.shared import Mm
from io import BytesIO

# === WebRTC audio capture ===
from streamlit_webrtc import webrtc_streamer, WebRtcMode, AudioProcessorBase
import av


# ==================== WORD REPORT ====================



def generate_word_report(template_path, context):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Word –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–±–ª–æ–Ω–∞ —Å –º–µ—Ç–∫–∞–º–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç docxtpl –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∑–∞–º–µ–Ω—ã –º–µ—Ç–æ–∫ –∏ –≤—Å—Ç–∞–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
    """
    try:
        doc = DocxTemplate(template_path)
        render_context = context.copy()

        # matplotlib figures -> InlineImage
        for key, value in context.items():
            if key.startswith("graph_") and value is not None:
                buf = BytesIO()
                value.savefig(buf, format="png", dpi=150, bbox_inches="tight")
                buf.seek(0)
                render_context[key] = InlineImage(doc, buf, width=Mm(150))
                plt.close(value)

        doc.render(render_context)

        doc_bytes = BytesIO()
        doc.save(doc_bytes)
        doc_bytes.seek(0)
        return doc_bytes

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
        raise


# ==================== –ù–ê–°–¢–†–û–ô–ö–ò –°–¢–†–ê–ù–ò–¶–´ ====================

st.set_page_config(
    page_title="–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞: LPC –∞–Ω–∞–ª–∏–∑ —Ä–µ—á–∏",
    page_icon="üé§",
    layout="wide"
)


# ==================== SESSION STATE ====================

def init_session_state():
    defaults = {
        "variant": 13,
        "student_name": "–ò–≤–∞–Ω–æ–≤ –ò.–ò.",
        "recordings": {},          # {'8000': AudioRecorder, '11025': AudioRecorder}
        "lpc_results": None,
        "lpc_params": None,
        "current_page": "–ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ",
        "audio_files_exist": False
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


init_session_state()


# ==================== WEBRTC AUDIO ====================

class SimpleAudioCollector(AudioProcessorBase):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –≤—Ö–æ–¥—è—â–∏–µ –∞—É–¥–∏–æ—Ñ—Ä–µ–π–º—ã (av.AudioFrame) –≤–æ –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã webrtc_streamer.
    """
    def __init__(self):
        self.frames = []

    def recv_audio(self, frame: av.AudioFrame) -> av.AudioFrame:
        self.frames.append(frame)
        return frame


def _frames_to_mono_float(frames) -> tuple[np.ndarray, int]:
    """
    frames (list[av.AudioFrame]) -> (mono float64 array, sample_rate)
    """
    if not frames:
        return np.array([], dtype=np.float64), 0

    chunks = []
    sr = frames[0].sample_rate or 0

    for fr in frames:
        sr = fr.sample_rate or sr
        arr = fr.to_ndarray()  # (channels, samples) typically
        chunks.append(arr)

    x = np.concatenate(chunks, axis=1) if chunks else np.zeros((1, 0), dtype=np.float32)

    # mono
    if x.ndim == 2 and x.shape[0] > 1:
        x = np.mean(x, axis=0)
    elif x.ndim == 2:
        x = x[0]
    else:
        x = x.astype(np.float64)

    x = x.astype(np.float64)

    # if input is int, normalize
    if np.issubdtype(x.dtype, np.integer):
        maxv = np.iinfo(x.dtype).max
        x = x / maxv

    # normalize peak a bit
    m = np.max(np.abs(x)) if x.size else 0.0
    if m > 0:
        x = 0.9 * x / m

    return x, int(sr) if sr else 0


def resample_and_fix_duration(x: np.ndarray, in_sr: int, out_sr: int, duration_s: Optional[int]) -> np.ndarray:
    """
    –†–µ—Å–µ–º–ø–ª + (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –æ–±—Ä–µ–∑–∫–∞/–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ duration_s —Å–µ–∫—É–Ω–¥.
    """
    if x.size == 0:
        x = np.zeros(0, dtype=np.float64)

    fs = in_sr if in_sr else out_sr
    y = x

    if fs != out_sr:
        y = signal.resample_poly(y, out_sr, fs)
        fs = out_sr

    if duration_s is not None and duration_s > 0:
        target_len = int(out_sr * duration_s)
        if y.size > target_len:
            y = y[:target_len]
        elif y.size < target_len:
            y = np.pad(y, (0, target_len - y.size))

    # final normalize
    m = np.max(np.abs(y)) if y.size else 0.0
    if m > 0:
        y = 0.9 * y / m

    return y.astype(np.float64)


def float_to_int16(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x, dtype=np.float64)
    x = np.clip(x, -1.0, 1.0)
    return (x * 32767.0).astype(np.int16)


# ==================== AUDIO CLASSES ====================

@dataclass
class AudioConfig:
    samplerate: int
    duration: int
    filename: str
    color: str = "blue"

    @property
    def total_samples(self) -> int:
        return self.duration * self.samplerate

    @property
    def size_kb(self) -> Optional[float]:
        if os.path.exists(self.filename):
            return os.path.getsize(self.filename) / 1024
        return None


class AudioRecorder:
    def __init__(self, config: AudioConfig):
        self.config = config
        self.rate: Optional[int] = None
        self.data: Optional[np.ndarray] = None
        self.bit_depth: Optional[int] = None

    def save_array(self, fs: int, data: np.ndarray) -> "AudioRecorder":
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç wav –Ω–∞ –¥–∏—Å–∫ –∏ –ø–æ–¥–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
        """
        write(self.config.filename, fs, data)
        return self.load()

    def load(self) -> "AudioRecorder":
        if os.path.exists(self.config.filename):
            self.rate, self.data = read(self.config.filename)

            if np.issubdtype(self.data.dtype, np.integer):
                self.bit_depth = self.data.dtype.itemsize * 8
            else:
                self.bit_depth = self.data.dtype.itemsize * 8
        return self

    def get_info(self) -> dict:
        if self.data is None or self.rate is None:
            return {}
        return {
            "samplerate": self.rate,
            "samples": len(self.data),
            "duration": len(self.data) / self.rate if self.rate else 0.0,
            "size_kb": self.config.size_kb,
            "bit_depth": self.bit_depth,
            "dtype": str(self.data.dtype)
        }

    def get_fragment(self, start_sample: int, end_sample: int) -> np.ndarray:
        if self.data is None:
            return np.array([])
        return self.data[start_sample:end_sample]

    def file_exists(self) -> bool:
        return os.path.exists(self.config.filename)

    def get_audio_bytes(self):
        if os.path.exists(self.config.filename):
            with open(self.config.filename, "rb") as f:
                return f.read()
        return None


# ==================== LPC FUNCTIONS ====================

def make_window(name: str, N: int):
    name = name.lower()
    if name in ("hann", "hanning"):
        return signal.windows.hann(N, sym=False)
    if name in ("hamming",):
        return signal.windows.hamming(N, sym=False)
    if name in ("rect", "rectangular", "boxcar"):
        return np.ones(N)
    raise ValueError("Unknown window. Use: hann, hamming, rect")


def frame_signal(x: np.ndarray, frame_len: int, hop: int):
    x = np.asarray(x, dtype=np.float64)
    n = len(x)
    if n < frame_len:
        x = np.pad(x, (0, frame_len - n))
        n = len(x)
    num = 1 + int(np.ceil((n - frame_len) / hop))
    pad = (num - 1) * hop + frame_len - n
    if pad > 0:
        x = np.pad(x, (0, pad))
    frames = np.stack([x[i * hop:i * hop + frame_len] for i in range(num)], axis=0)
    return frames


def overlap_add(frames: np.ndarray, hop: int):
    num, frame_len = frames.shape
    out_len = (num - 1) * hop + frame_len
    y = np.zeros(out_len, dtype=np.float64)
    for i in range(num):
        y[i * hop:i * hop + frame_len] += frames[i]
    return y


def levinson_durbin(r: np.ndarray, order: int):
    r = np.asarray(r, dtype=np.float64)
    if len(r) < order + 1:
        raise ValueError("r must have length >= order+1")

    if r[0] <= 1e-12:
        return np.zeros(order), 0.0, np.zeros(order)

    a = np.zeros(order + 1, dtype=np.float64)
    e = r[0]
    a[0] = 1.0
    k = np.zeros(order, dtype=np.float64)

    for i in range(1, order + 1):
        acc = 0.0
        for j in range(1, i):
            acc += a[j] * r[i - j]
        ki = -(r[i] + acc) / e
        k[i - 1] = ki

        a_new = a.copy()
        for j in range(1, i):
            a_new[j] = a[j] + ki * a[i - j]
        a_new[i] = ki
        a = a_new

        e *= (1.0 - ki * ki)
        if e < 1e-12:
            e = 1e-12

    return a[1:], e, k


def autocorr(x: np.ndarray, order: int):
    x = np.asarray(x, dtype=np.float64)
    r_full = np.correlate(x, x, mode="full")
    mid = len(r_full) // 2
    r = r_full[mid:mid + order + 1]
    return r


def lpc_encode_frames(frames: np.ndarray, order: int):
    num, frame_len = frames.shape
    A = np.zeros((num, order), dtype=np.float64)
    E = np.zeros(num, dtype=np.float64)
    R_frames = np.zeros_like(frames)

    for i in range(num):
        x = frames[i]
        r = autocorr(x, order)
        a, e, _ = levinson_durbin(r, order)
        A[i] = a
        E[i] = e
        R_frames[i] = signal.lfilter(np.r_[1.0, A[i]], [1.0], x)

    return A, E, R_frames


def lpc_synthesize_frames(A: np.ndarray, E: np.ndarray, frame_len: int, excitation="noise"):
    num, order = A.shape
    frames_hat = np.zeros((num, frame_len), dtype=np.float64)

    for i in range(num):
        if excitation == "noise":
            src = np.random.randn(frame_len) * np.sqrt(max(E[i], 1e-12))
        else:
            raise ValueError("Only 'noise' excitation implemented")

        den = np.r_[1.0, A[i]]
        frames_hat[i] = signal.lfilter([1.0], den, src)

    return frames_hat


def run_lpc_codec(
    audio_data: np.ndarray,
    original_fs: int,
    target_fs=8000,
    frame_ms=30,
    overlap=0.5,
    window_name="hann",
    order=10
):
    x = audio_data.copy()

    if np.issubdtype(x.dtype, np.integer):
        maxv = np.iinfo(x.dtype).max
        x = x.astype(np.float64) / maxv
    else:
        x = x.astype(np.float64)

    if np.max(np.abs(x)) > 0:
        x = 0.9 * x / np.max(np.abs(x))

    fs = original_fs
    if target_fs is not None and fs != target_fs:
        x = signal.resample_poly(x, target_fs, fs)
        fs = target_fs

    frame_len = int(round(frame_ms * 1e-3 * fs))
    hop = int(round(frame_len * (1.0 - overlap)))
    if hop <= 0:
        raise ValueError("Overlap too large -> hop <= 0")

    w = make_window(window_name, frame_len)

    frames = frame_signal(x, frame_len, hop)
    frames_w = frames * w[None, :]

    A, E, R_frames = lpc_encode_frames(frames_w, order)
    frames_hat = lpc_synthesize_frames(A, E, frame_len, excitation="noise")

    y = overlap_add(frames_hat, hop)
    r = overlap_add(R_frames, hop)

    y = y[:len(x)]
    r = r[:len(x)]

    t = np.arange(len(x)) / fs

    if np.max(np.abs(y)) > 0:
        y = 0.9 * y / np.max(np.abs(y))

    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
    write(temp_file.name, fs, float_to_int16(y))

    return {
        "fs": fs,
        "x": x,
        "y": y,
        "r": r,
        "A": A,
        "E": E,
        "output_file": temp_file.name,
        "frame_len": frame_len,
        "hop": hop,
        "num_frames": A.shape[0]
    }


def plot_residual_vs_order(audio_data, original_fs, target_fs=8000, frame_ms=30, frame_number=10, max_order=20):
    x = audio_data.copy()

    if np.issubdtype(x.dtype, np.integer):
        maxv = np.iinfo(x.dtype).max
        x = x.astype(np.float64) / maxv
    else:
        x = x.astype(np.float64)

    fs = original_fs
    if fs != target_fs:
        x = signal.resample_poly(x, target_fs, fs)
        fs = target_fs

    frame_len = int(round(frame_ms * 1e-3 * fs))
    hop = int(round(frame_len * 0.5))

    frames = frame_signal(x, frame_len, hop)
    w = make_window("hann", frame_len)
    frames_w = frames * w[None, :]

    if frame_number >= len(frames_w):
        frame_number = len(frames_w) - 1

    frame_data = frames_w[frame_number]

    orders = range(1, max_order + 1)
    residual_powers = []

    for order in orders:
        r = autocorr(frame_data, order)
        if r[0] > 1e-12:
            _, e, _ = levinson_durbin(r, order)
            residual_powers.append(e)
        else:
            residual_powers.append(0)

    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(list(orders), residual_powers, "b-o", linewidth=2, markersize=8)
    ax.set_xlabel("–ü–æ—Ä—è–¥–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (p)")
    ax.set_ylabel("–ú–æ—â–Ω–æ—Å—Ç—å –æ—Å—Ç–∞—Ç–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è")
    ax.set_title(f"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–æ—â–Ω–æ—Å—Ç–∏ –æ—Å—Ç–∞—Ç–∫–∞ –æ—Ç –ø–æ—Ä—è–¥–∫–∞ LPC\n–§—Ä–µ–π–º #{frame_number}")
    ax.grid(True, alpha=0.3)

    return fig, list(orders), residual_powers


# ==================== PLOTS ====================

def create_signal_plot(data, samplerate, title, color="blue"):
    duration = len(data) / samplerate if samplerate else 0
    time_arr = np.linspace(0, duration, len(data)) if len(data) else np.array([])

    fig, ax = plt.subplots(figsize=(10, 4))
    ax.plot(time_arr, data, color=color, linewidth=0.5)
    ax.grid(True, alpha=0.3)
    ax.set_title(title)
    ax.set_xlabel("–í—Ä–µ–º—è (—Å)")
    ax.set_ylabel("–ê–º–ø–ª–∏—Ç—É–¥–∞")
    return fig


def create_waveform_plots(t, x_in, x_out, x_res, title_prefix=""):
    figs = []

    fig1, ax1 = plt.subplots(figsize=(10, 4))
    ax1.plot(t, x_in, "b-", linewidth=0.5)
    ax1.set_title(f"{title_prefix}–í—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª (–æ—Å—Ü–∏–ª–ª–æ–≥—Ä–∞–º–º–∞)")
    ax1.set_xlabel("–í—Ä–µ–º—è, —Å")
    ax1.set_ylabel("–ê–º–ø–ª–∏—Ç—É–¥–∞")
    ax1.grid(True, alpha=0.3)
    figs.append(fig1)

    fig2, ax2 = plt.subplots(figsize=(10, 4))
    ax2.plot(t[:len(x_out)], x_out, "r-", linewidth=0.5)
    ax2.set_title(f"{title_prefix}–°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª (–æ—Å—Ü–∏–ª–ª–æ–≥—Ä–∞–º–º–∞)")
    ax2.set_xlabel("–í—Ä–µ–º—è, —Å")
    ax2.set_ylabel("–ê–º–ø–ª–∏—Ç—É–¥–∞")
    ax2.grid(True, alpha=0.3)
    figs.append(fig2)

    fig3, ax3 = plt.subplots(figsize=(10, 4))
    ax3.plot(t[:len(x_res)], x_res, "g-", linewidth=0.5)
    ax3.set_title(f"{title_prefix}–û—Å—Ç–∞—Ç–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–æ—Å—Ü–∏–ª–ª–æ–≥—Ä–∞–º–º–∞)")
    ax3.set_xlabel("–í—Ä–µ–º—è, —Å")
    ax3.set_ylabel("–ê–º–ø–ª–∏—Ç—É–¥–∞")
    ax3.grid(True, alpha=0.3)
    figs.append(fig3)

    return figs


def create_spectrogram(x, fs, title, nperseg=256, noverlap=192):
    f, tt, Sxx = signal.spectrogram(
        x, fs=fs, window="hann", nperseg=nperseg, noverlap=noverlap,
        scaling="spectrum", mode="magnitude"
    )

    fig, ax = plt.subplots(figsize=(10, 4))
    pcm = ax.pcolormesh(tt, f, 20 * np.log10(Sxx + 1e-12), shading="auto", cmap="viridis")
    ax.set_ylim(0, fs / 2)
    ax.set_title(title)
    ax.set_xlabel("–í—Ä–µ–º—è, —Å")
    ax.set_ylabel("–ß–∞—Å—Ç–æ—Ç–∞, –ì—Ü")
    plt.colorbar(pcm, ax=ax, label="–£—Ä–æ–≤–µ–Ω—å, dB")
    return fig


# ==================== STREAMLIT UI ====================

def run():
    st.sidebar.title("–ù–∞–≤–∏–≥–∞—Ü–∏—è")
    pages = ["–ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ", "LPC –∞–Ω–∞–ª–∏–∑", "–û—Ç—á–µ—Ç"]
    st.session_state.current_page = st.sidebar.radio("–ü–µ—Ä–µ–π—Ç–∏ –∫:", pages)

    st.sidebar.title("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    st.session_state.variant = st.sidebar.number_input(
        "–ù–æ–º–µ—Ä –≤–∞—Ä–∏–∞–Ω—Ç–∞", min_value=1, max_value=30, value=st.session_state.variant
    )
    st.session_state.student_name = st.sidebar.text_input("–§–ò–û —Å—Ç—É–¥–µ–Ω—Ç–∞", value=st.session_state.student_name)

    st.sidebar.markdown("---")
    st.sidebar.subheader("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã LPC –∞–Ω–∞–ª–∏–∑–∞")
    frame_ms = st.sidebar.slider("–†–∞–∑–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞ (–º—Å)", min_value=10, max_value=50, value=30, step=5)
    overlap = st.sidebar.slider("–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ", min_value=0.0, max_value=0.9, value=0.5, step=0.1)
    lpc_order = st.sidebar.slider("–ü–æ—Ä—è–¥–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", min_value=2, max_value=20, value=10, step=1)

    if st.session_state.current_page == "–ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ":
        show_recording_page()
    elif st.session_state.current_page == "LPC –∞–Ω–∞–ª–∏–∑":
        show_lpc_analysis_page(frame_ms, overlap, lpc_order)
    elif st.session_state.current_page == "–û—Ç—á–µ—Ç":
        show_report_page(frame_ms, overlap, lpc_order)


def show_recording_page():
    # –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫–∞–∫ –≤ —Ç–≤–æ—ë–º –∫–æ–¥–µ (–µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏–º)
    for img in ("prikol.jpg", "prikol1.jpg"):
        if os.path.exists(img):
            st.image(img)

    st.title("üé§ –ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤")
    st.markdown(
        "–î–∞–Ω–Ω–∞—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å–æ —Ñ–∞–π–ª–æ–º .wav (–ø–æ–ª—É—á–µ–Ω–∏–µ –µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤), "
        "—Ä–∞–±–æ—Ç—É —Å LPC-–∫–æ–¥–µ–∫–æ–º. –î–ª—è —ç—Ç–æ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø–∏—Å–∞—Ç—å –¥–≤–∞ –∑–≤—É–∫–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–∞ –≤ —Ä–∞–∑–¥–µ–ª–µ '–ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ'. "
        "–î–∞–ª–µ–µ ‚Äî 'LPC-–∞–Ω–∞–ª–∏–∑' –∏ –∑–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞. –ü–æ—Ç–æ–º ‚Äî '–û—Ç—á–µ—Ç' –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è Word.\n\n"
        "–í–ù–ò–ú–ê–ù–ò–ï: –í–∞—Ä–∏–∞–Ω—Ç–æ–º, –ø–æ —Å—É—Ç–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏—à—å —Ñ–∞–º–∏–ª–∏—è."
    )

    st.markdown("""
    ### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
    1. –ù–∞–∂–º–∏—Ç–µ **Start** –≤ –Ω—É–∂–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ
    2. –î–æ–∂–¥–∏—Ç–µ—Å—å –ø–æ–∫–∞ –∫–Ω–æ–ø–∫–∞ **Stop** —Å—Ç–∞–Ω–µ—Ç —Å –∫—Ä–∞—Å–Ω—ã–º —Ñ–æ–Ω–æ–º
    3. –ü—Ä–æ–∏–∑–Ω–µ—Å–∏—Ç–µ —Ñ–∞–º–∏–ª–∏—é (–ø—Ä–∏–º–µ—Ä–Ω–æ 5 —Å–µ–∫—É–Ω–¥, –º–æ–∂–Ω–æ —á—É—Ç—å –±–æ–ª—å—à–µ)
    4. –ù–∞–∂–º–∏—Ç–µ **–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–∞–ø–∏—Å—å**
    5. –ù–∞–∂–º–∏—Ç–µ **Stop**
    """)

    col1, col2 = st.columns(2)

    def webrtc_record_block(title: str, key_prefix: str, target_sr: int, filename: str, color: str, state_key: str):
        st.subheader(title)

        # –≥–¥–µ —Ö—Ä–∞–Ω–∏–º –∫–∞–¥—Ä—ã –º–µ–∂–¥—É –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–º–∏
        frames_key = f"{key_prefix}_frames"
        if frames_key not in st.session_state:
            st.session_state[frames_key] = []

        # –∫–Ω–æ–ø–∫–∞ "–æ—á–∏—Å—Ç–∏—Ç—å –∑–∞–ø–∏—Å—å"
        if st.button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –∑–∞–ø–∏—Å—å", key=f"{key_prefix}_clear"):
            st.session_state[frames_key] = []
            st.success("–ë—É—Ñ–µ—Ä –∑–∞–ø–∏—Å–∏ –æ—á–∏—â–µ–Ω.")

        ctx = webrtc_streamer(
            key=f"{key_prefix}_webrtc",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            media_stream_constraints={
                "audio": {
                    "echoCancellation": False,
                    "noiseSuppression": False,
                    "autoGainControl": False,
                    "channelCount": 1
                },
                "video": False
            },
            async_processing=True,
        )

        st.caption("–ó–∞–ø–∏—Å—å –∏–¥—ë—Ç –≤ –±—Ä–∞—É–∑–µ—Ä–µ: Start ‚Üí –≥–æ–≤–æ—Ä–∏—Ç–µ ‚Üí Stop ‚Üí –°–æ—Ö—Ä–∞–Ω–∏—Ç—å.")

        # –í–ê–ñ–ù–û: –ø–æ–∫–∞ –∏–¥–µ—Ç –∑–∞–ø–∏—Å—å, –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Ñ—Ä–µ–π–º—ã –∏–∑ audio_receiver –∏ –∫–æ–ø–∏–º –≤ session_state
        if ctx and ctx.state.playing and ctx.audio_receiver:
            try:
                while True:
                    audio_frames = ctx.audio_receiver.get_frames(timeout=0.01)
                    if not audio_frames:
                        break
                    st.session_state[frames_key].extend(audio_frames)
            except Exception:
                # —Ç–∞–π–º–∞—É—Ç—ã/–ø—É—Å—Ç—ã–µ –æ—á–µ—Ä–µ–¥–∏ ‚Äî –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è
                pass

        st.write(f"–ö–∞–¥—Ä–æ–≤ –≤ –±—É—Ñ–µ—Ä–µ: {len(st.session_state[frames_key])}")

        if st.button(f"üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–∞–ø–∏—Å—å ({target_sr} –ì—Ü)", key=f"{key_prefix}_save"):
            frames = st.session_state[frames_key]
            if not frames:
                st.warning("–ë—É—Ñ–µ—Ä –ø—É—Å—Ç. –ù–∞–∂–º–∏—Ç–µ Start, –ø–æ–≥–æ–≤–æ—Ä–∏—Ç–µ 5 —Å–µ–∫—É–Ω–¥, Stop ‚Äî –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
                return

            # frames -> mono float -> —Ä–µ—Å–µ–º–ø–ª -> int16 wav
            x, in_sr = _frames_to_mono_float(frames)
            if x.size == 0:
                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞—É–¥–∏–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø–∏—Å–∞—Ç—å –µ—â—ë —Ä–∞–∑.")
                return

            x_rs = resample_and_fix_duration(x, in_sr=in_sr, out_sr=target_sr, duration_s=5)
            x16 = float_to_int16(x_rs)
            x_rs = np.clip(x_rs, -1.0, 1.0)
            x32 = x_rs.astype(np.float32)

            config = AudioConfig(target_sr, 5, filename, color)
            recorder = AudioRecorder(config).save_array(target_sr, x32)
            st.session_state.recordings[state_key] = recorder

            st.success("–ó–∞–ø–∏—Å—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

        # –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ
        if state_key in st.session_state.recordings:
            recorder = st.session_state.recordings[state_key]
            info = recorder.get_info()

            st.write("**–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:**")
            st.write(f"- –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {info.get('duration', 0):.2f} —Å")
            st.write(f"- –†–∞–∑–º–µ—Ä: {info.get('size_kb', 0):.2f} –ö–ë")
            st.write(f"- –ì–ª—É–±–∏–Ω–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è: {info.get('bit_depth', 'N/A')} –±–∏—Ç")

            audio_bytes = recorder.get_audio_bytes()
            if audio_bytes:
                st.audio(audio_bytes, format="audio/wav")
                st.download_button(
                    label=f"üì• –°–∫–∞—á–∞—Ç—å ({target_sr} –ì—Ü)",
                    data=audio_bytes,
                    file_name=os.path.basename(filename),
                    mime="audio/wav",
                    key=f"{key_prefix}_download"
                )

            fig = create_signal_plot(recorder.data, recorder.rate, f"–°–∏–≥–Ω–∞–ª ({target_sr} –ì—Ü)", color)
            st.pyplot(fig)
            plt.close(fig)

    with col1:
        webrtc_record_block("–ó–∞–ø–∏—Å—å 1 (8000 –ì—Ü)", "rec8000", 8000, "output.wav", "blue", "8000")

    with col2:
        webrtc_record_block("–ó–∞–ø–∏—Å—å 2 (11025 –ì—Ü)", "rec11025", 11025, "output11025.wav", "green", "11025")

    # –§—Ä–∞–≥–º–µ–Ω—Ç —Å–∏–≥–Ω–∞–ª–∞ (–∫–∞–∫ —É —Ç–µ–±—è)
    st.markdown("---")
    st.subheader("–§—Ä–∞–≥–º–µ–Ω—Ç —Å–∏–≥–Ω–∞–ª–∞")

    if "8000" in st.session_state.recordings:
        recorder = st.session_state.recordings["8000"]

        start_sample = 5000
        end_sample = 1000 * st.session_state.variant
        if start_sample >= end_sample:
            start_sample = 4000
            end_sample = 5000

        fragment = recorder.get_fragment(start_sample, end_sample)

        st.write("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞:**")
        st.write(f"- –ù–∞—á–∞–ª—å–Ω—ã–π —Å—ç–º–ø–ª: {start_sample}")
        st.write(f"- –ö–æ–Ω–µ—á–Ω—ã–π —Å—ç–º–ø–ª: {end_sample}")
        st.write(f"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—ç–º–ø–ª–æ–≤: {len(fragment)}")
        st.write(f"- –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {len(fragment) / recorder.rate:.3f} —Å")

        fig = create_signal_plot(fragment, recorder.rate, f"–§—Ä–∞–≥–º–µ–Ω—Ç —Å–∏–≥–Ω–∞–ª–∞ (—Å—ç–º–ø–ª—ã {start_sample}-{end_sample})", "red")
        st.pyplot(fig)
        plt.close(fig)


def show_lpc_analysis_page(frame_ms, overlap, lpc_order):
    st.title("üî¨ LPC –∞–Ω–∞–ª–∏–∑ –∏ —Å–∏–Ω—Ç–µ–∑ —Ä–µ—á–∏")

    if "11025" not in st.session_state.recordings:
        st.warning("–°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∑–∞–ø–∏—Å—å –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ '–ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ'")
        return

    recorder = st.session_state.recordings["11025"]

    st.write(f"**–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π —Ñ–∞–π–ª:** {recorder.config.filename}")
    st.write(f"**–ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏:** {recorder.rate} –ì—Ü")
    st.write("**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞:**")
    st.write(f"- –†–∞–∑–º–µ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞: {frame_ms} –º—Å")
    st.write(f"- –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {overlap:.1%}")
    st.write(f"- –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {lpc_order}")

    if st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å LPC –∞–Ω–∞–ª–∏–∑", type="primary"):
        with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è LPC –∞–Ω–∞–ª–∏–∑..."):
            results = run_lpc_codec(
                audio_data=recorder.data,
                original_fs=recorder.rate,
                target_fs=8000,
                frame_ms=frame_ms,
                overlap=overlap,
                window_name="hann",
                order=lpc_order
            )

            st.session_state.lpc_results = results
            st.session_state.lpc_params = {
                "frame_ms": frame_ms,
                "overlap": overlap,
                "lpc_order": lpc_order
            }

            st.success("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
            show_lpc_results(results)

    elif st.session_state.lpc_results is not None:
        show_lpc_results(st.session_state.lpc_results)

    st.markdown("---")
    st.subheader("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Å—Ç–∞—Ç–∫–∞ –æ—Ç –ø–æ—Ä—è–¥–∫–∞ LPC")

    col1, col2 = st.columns(2)
    with col1:
        frame_number = st.number_input("–ù–æ–º–µ—Ä —Ñ—Ä–µ–π–º–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞", min_value=0, value=10, step=1)
    with col2:
        max_order = st.number_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫", min_value=5, max_value=30, value=20, step=1)

    if st.button("üìä –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"):
        fig, orders, powers = plot_residual_vs_order(
            audio_data=recorder.data,
            original_fs=recorder.rate,
            target_fs=8000,
            frame_ms=frame_ms,
            frame_number=frame_number,
            max_order=max_order
        )

        st.pyplot(fig)
        plt.close(fig)

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ—Ç—á—ë—Ç–∞
        st.session_state.residual_plot = fig

        df = pd.DataFrame({
            "–ü–æ—Ä—è–¥–æ–∫": orders,
            "–ú–æ—â–Ω–æ—Å—Ç—å –æ—Å—Ç–∞—Ç–∫–∞": powers,
            "–û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ": [1 - p / powers[0] if powers[0] > 0 else 0 for p in powers]
        })
        st.dataframe(df)


def show_lpc_results(results):
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("–ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏", f"{results['fs']} –ì—Ü")
    with col2:
        st.metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–µ–π–º–æ–≤", results["num_frames"])
    with col3:
        st.metric("–î–ª–∏–Ω–∞ —Ñ—Ä–µ–π–º–∞", f"{results['frame_len']} —Å—ç–º–ø–ª–æ–≤")

    st.subheader("–û—Å—Ü–∏–ª–ª–æ–≥—Ä–∞–º–º—ã")
    t = np.arange(len(results["x"])) / results["fs"]
    figs = create_waveform_plots(t, results["x"], results["y"], results["r"])

    tab1, tab2, tab3 = st.tabs(["–ò—Å—Ö–æ–¥–Ω—ã–π", "–°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π", "–û—Å—Ç–∞—Ç–æ–∫"])
    with tab1:
        st.pyplot(figs[0])
    with tab2:
        st.pyplot(figs[1])
    with tab3:
        st.pyplot(figs[2])

    st.session_state.osc_plots = figs

    st.subheader("–°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã")
    spec_figs = [
        create_spectrogram(results["x"], results["fs"], "–°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"),
        create_spectrogram(results["y"], results["fs"], "–°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞"),
        create_spectrogram(results["r"], results["fs"], "–°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞—Ç–∫–∞"),
    ]

    tab1, tab2, tab3 = st.tabs(["–ò—Å—Ö–æ–¥–Ω—ã–π", "–°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π", "–û—Å—Ç–∞—Ç–æ–∫"])
    with tab1:
        st.pyplot(spec_figs[0])
    with tab2:
        st.pyplot(spec_figs[1])
    with tab3:
        st.pyplot(spec_figs[2])

    st.session_state.spect_plots = spec_figs

    st.subheader("–°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª")
    if os.path.exists(results["output_file"]):
        with open(results["output_file"], "rb") as f:
            audio_bytes = f.read()
            st.audio(audio_bytes, format="audio/wav")
            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª",
                data=audio_bytes,
                file_name=f"synthesized_{st.session_state.lpc_params['lpc_order']}.wav",
                mime="audio/wav"
            )

    st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    total_params = results["A"].size + results["E"].size
    compression_ratio = len(results["x"]) / total_params if total_params > 0 else 0

    col1, col2 = st.columns(2)
    with col1:
        st.write(f"**–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ (A+E):** {total_params}")
        st.write(f"**–°—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è:** {compression_ratio:.2f} —Å—ç–º–ø–ª–æ–≤/–ø–∞—Ä–∞–º–µ—Ç—Ä")

    st.session_state.compression_ratio = compression_ratio
    st.session_state.total_params = total_params


def show_report_page(frame_ms, overlap, lpc_order):
    st.title("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞")

    required_data = [
        "8000" in st.session_state.recordings,
        "11025" in st.session_state.recordings,
        st.session_state.lpc_results is not None
    ]
    if not all(required_data):
        st.warning("–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø–∏—Å—å –æ–±–æ–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏ LPC –∞–Ω–∞–ª–∏–∑")
        return

    template_path = "pattern_rad_lab1.docx"
    if not os.path.exists(template_path):
        st.error(f"‚ùå –§–∞–π–ª —à–∞–±–ª–æ–Ω–∞ '{template_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏!")
        with st.expander("üìÅ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"):
            for file in os.listdir("."):
                st.write(f"- {file}")
        return

    st.success("‚úÖ –®–∞–±–ª–æ–Ω –Ω–∞–π–¥–µ–Ω")

    if st.button("üì• –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç Word", type="primary"):
        with st.spinner("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞..."):
            try:
                recorder_8000 = st.session_state.recordings["8000"]
                recorder_11025 = st.session_state.recordings["11025"]
                lpc_results = st.session_state.lpc_results

                info_8000 = recorder_8000.get_info()
                info_11025 = recorder_11025.get_info()

                start_sample = 5000
                end_sample = 1000 * st.session_state.variant
                if start_sample >= end_sample:
                    start_sample = 4000
                    end_sample = 5000

                fragment = recorder_8000.get_fragment(start_sample, end_sample)

                st.info("üîÑ –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –≥—Ä–∞—Ñ–∏–∫–∏...")

                graph_8000 = create_signal_plot(recorder_8000.data, recorder_8000.rate, "–ü–æ–ª–Ω—ã–π —Å–∏–≥–Ω–∞–ª (8000 –ì—Ü)", "blue")
                graph_frag = create_signal_plot(fragment, recorder_8000.rate, "–§—Ä–∞–≥–º–µ–Ω—Ç —Å–∏–≥–Ω–∞–ª–∞", "red")
                graph_11025 = create_signal_plot(recorder_11025.data, recorder_11025.rate, "–ü–æ–ª–Ω—ã–π —Å–∏–≥–Ω–∞–ª (11025 –ì—Ü)", "green")

                total_params = lpc_results["A"].size + lpc_results["E"].size
                compression_ratio = len(lpc_results["x"]) / total_params if total_params > 0 else 0

                context = {
                    "name": st.session_state.student_name,

                    "size_kb_8000": f"{info_8000.get('size_kb', 0):.2f}",
                    "bit_depth_8000": str(info_8000.get("bit_depth", "N/A")),
                    "graph_8000": graph_8000,

                    "start_sample": str(start_sample),
                    "end_sample": str(end_sample),
                    "len_fragment": str(len(fragment)),
                    "time_frag": f"{len(fragment) / recorder_8000.rate:.3f}",
                    "graph_frag": graph_frag,

                    "size_kb_11025": f"{info_11025.get('size_kb', 0):.2f}",
                    "bit_depth_11025": str(info_11025.get("bit_depth", "N/A")),
                    "graph_11025": graph_11025,

                    # LPC —á–∞—Å—Ç—å
                    "fs_lpc": lpc_results["fs"],
                    # –∏–Ω–æ–≥–¥–∞ –≤ —à–∞–±–ª–æ–Ω–∞—Ö/—Å–ø–∏—Å–∫–∞—Ö –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –æ–ø–µ—á–∞—Ç–∫–∞ fs_lps ‚Äî –ø–æ–¥—Å—Ç—Ä–∞—Ö—É–µ–º—Å—è:
                    "fs_lps": lpc_results["fs"],

                    "frame_ms": str(frame_ms),
                    "frame_sem": str(lpc_results["frame_len"]),
                    "overlap": f"{overlap:.1%}",
                    "full_frame": str(lpc_results["num_frames"]),
                    "full_sem": str(len(lpc_results["x"])),
                    "lpc_order": str(lpc_order),
                    "count_order": str(total_params),
                    "coeff": f"{compression_ratio:.2f}",
                }

                if hasattr(st.session_state, "osc_plots") and st.session_state.osc_plots and len(st.session_state.osc_plots) >= 3:
                    context["graph_osc_orig"] = st.session_state.osc_plots[0]
                    context["graph_osc_sint"] = st.session_state.osc_plots[1]
                    context["graph_osc_frag"] = st.session_state.osc_plots[2]

                if hasattr(st.session_state, "spect_plots") and st.session_state.spect_plots and len(st.session_state.spect_plots) >= 3:
                    context["graph_spect_orig"] = st.session_state.spect_plots[0]
                    context["graph_spect_sint"] = st.session_state.spect_plots[1]
                    context["graph_spect_frag"] = st.session_state.spect_plots[2]

                if hasattr(st.session_state, "residual_plot") and st.session_state.residual_plot:
                    context["graph_zavis_lpc"] = st.session_state.residual_plot

                st.info("üìÑ –§–æ—Ä–º–∏—Ä—É—é –æ—Ç—á–µ—Ç...")
                doc_bytes = generate_word_report(template_path, context)

                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å –æ—Ç—á–µ—Ç",
                    data=doc_bytes,
                    file_name=f"LPC_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
                )

                plt.close("all")
                st.success("‚úÖ –û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω!")

            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
                import traceback
                with st.expander("–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏"):
                    st.code(traceback.format_exc())


